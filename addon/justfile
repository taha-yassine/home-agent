dev:
    uv run uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload

phoenix:
    uv run phoenix serve

vllm tool_parser='json':
    docker run -it --rm -p 8080:8000 -v ~/.cache/huggingface:/root/.cache/huggingface -v $(pwd)/templates:/workspace/templates vllm-cpu-env:latest --model meta-llama/Llama-3.2-3B-Instruct --max-model-len 8192 --enable-auto-tool-choice --tool-call-parser {{ if tool_parser == "json" { "llama3_json" } else { "pythonic" } }} --chat-template /workspace/templates/tool_chat_template_llama3.2_{{tool_parser}}.jinja

llama_cpp model='models/Llama-3.2-3B-Instruct.Q8_0.gguf':
    llama-server --port 8080 --jinja -m {{model}}
